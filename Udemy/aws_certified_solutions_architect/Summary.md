# Intro to Security Group

# Launching an Apache Server on EC2

# EC2 User Data

# EC2 Instance Launch Type
- On Demand : short workload, predictable pricing
- Reserved Instances: long worklogads
- Convertible Reserved Instances: 


# EC2 for Solutions Architects
- EC2 instances are billed by the second, t3.micro is free tier
- On Linux / Mac we use ssh, on Windows we use Putty
- SSH is on port 23, lock down the security group to your IP
- Know the 4 EC2 launch modes:  
    - On demand
    - Reserved
    - Spot instances
    - Dedicated hosts

# Scalability & High Availability 

# What is load balancing?
- Load balancers are servers that forward internet traffic to multiple serers downstream

- Application Load Balancer (v2)
    - Load balancing to multiple HTTP applications across machines (target group)
    - Load balancing to multiple applications on the same machine 
    - Load balancing based on route in URL
    - Load balancing based on hostnmae in URL

- Basically, they're awesome for micro services & container-based application
- Has a port mapping feature to redirect to a 
dynamic port

- ALB (v2) good to know
    - Stickiness can be enabled at the target group level
        - Same request goes to the same instance
        - Stickiness is directly generated by the ALB
    - ALB support HTTP/HTTPS & Websockets protocols
    -  The application servers don't see the IP of the client directly
        - The true IP of the client is inserted in the header X-Forwarded-For
        - We can also get Port and proto

- Network Load Balancer (v2)
    - Network load balancers (L4)
        - Forward TCP traffic to your instances
        - Handle millions of request per seconds
        - Support for static IP or elastic IP
        - Less latency ~ 100 ms (vs 400 ms for ALB)
    - Network load balancers are mostly used for extreme performance and should not be the default load balancer you choose


- LB : Good to Know
    - CLB and ALB support SSL sertificates and provide SSL termination
    - All load balacners have health check capability
    - ALB can route on based on hostname / path
    - ALB is a great fit with ECS
    - Any load balancer has a static host name. Do not resolve and use underlying ip


# Load Balancer Stickiness
- It is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer
- This works for Classic Load Balancers & ALB
- The 'Cookie' used for stickiness has an expiration date you control
- Use case: make sure the user doesn't lose his session data
- Enabling stickiness may bring imblanace to the load over the backend EC2 instances
    - Target group's stickiness control


# Load Balancers for Solutions Architect
- Classic Load Balancers: questions on security groups, stickiness
- Application Load Balancer (Layer 7 of OSI)
    - Support routing based on hostname (users.example.com & payments.example.com)
    - Support routing based on path (example.com/users & example.com/payments)
    - Support redirects (from HTTP to HTTPS for example)
    - Support dynamic host port mapping with ECS
- NLB (Layer 4 of OSI) gets a static IP per AZ
    - Public facing: must attach Elastic IP - can help whitelist by clients
    - Private facing: will get random pricate IP based on free ones at time of creation
    - Has cross zone balancing
    - Has SSL termination (Jab 2019)

- Load Balancer Securit Groups
    - 80, 443 open for LB
    - only 80 open for the target instances

- LB - SSL Certificates
    - Users - LB : HTTPS(encrypted) over www
    - LB - EC2 instances : HTTP over private VPC
    - the LB uses an x.509 certificate (SSL/TLS server certificate)
    - You can manage certificates using ACM (AWS certificate Manager)
    - You can create and upload your own certificates alternatively
    - HTTPS listener:
        - you must specify a default certificate
        - You can add an optional list of certs to support multiple domains
        - clients can use SNI (Server Name Indication) to specify the hostname they reach
            - https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/
        - Ability to specify a security policy to support older versions of SSL / TLS (legacy clients) 

- Auto Scaling Groups Overview

- ASGs attributes
    - A launch configuration
        - AMI + instance type
        - EC2 User Data
        - EBS volumes
        - Security Groups
        - SSH key pair
    - Min size / max size/ initial capacity
    - network + subnets info
    - LB info
    - Scaling Policies

- Auto Scaling Alarms
    - it is possible to scale an ASG based on CloudWatch alamrs
    - Metrics are computed a metric (such as Average CPU)
    - Based on the alarm
        - We can create scale-out policies (increase the number of instances)
        - We can create scale-in policies (decrease the number of instances)

- Auto Scaling New Rules
    - It is now possible to define better auto scaling rules that are directly managed by EC2
        - Target Average CPU Usage
        - Number of reqeusts on the ELB per instance
        - Average Network In
        - Average Network Out
    - These rules are easier to set up and can make more sense

- Auto Scaling Custom Metric
    - We can auto scale based on a custom metric
    - 1. Send custom metric from application on EC2 to CloudWatch
    - 2. Create CloudWatch alarm to react to low / high values
    - 3. Use the CloudWatch alarm as the scaling policy for ASG


# ASG for Solutions Architects
- ASG Default Termination Policy
    - 1. Find the AZ which has the most number of instances
    - 2. If there are multiple instances in the AZ to choose from, delete the one with the oldest launch configuration
- ASG tries the balance the number of instances across AZ by default

- Scaling Cooldowns
    - The cooldown period helps to ensure that your Auto Scaling group doesn't launch or terminate additional instances before the previous scaling activity takes effect
    - In addtiion to default cooldown for Auto Scaling Group, we can create cooldowns that apply to a specific simple scaling policy
    - A scaling-specific cooldown period overrides the default cooldown period
    - One common use for scaling-specific cooldowns is with a scale-in policy - a policy that terminates instances based on a specific criteria or metric. Because this policy terminates instances, Amazon EC2 Auto Scaling needs less time to determine whether to terminate additional instances.
    - If the default cooldown period of 300 seconds is too long - you can reduce costs by applying a scaling-specific cooldown period of 180 seconds to the scale-in policy
    - If your application is scaling up and down multiple times each hour, modify the Auto Scaling Groups cool-down timers and the CloudWatch Alarm Period that triggers the scale in

# EBS Intro
- EBS Volume
    - it's a network driver
        - It uses the network to communicate the instance, which means there might be a bit of latency
        - It can be dtached from an EC2 instance and attached to another one quickly
    - It's locked to an Availability Zone
        - An EBS volume in us-east-1  a cannot be attchedto us-east-1 b
        - To move a volume across, you first need to snapshot it
    - Have a provisioned capacity
        - You get billed for all the provisioned capacity

- EBS Volume Types
    - GP2 (SSD) : General purpose SSD volume that balances price and performance for a wide variety of workloads
    - IOI (SSD) : Highest -performance SSD volume for mission-critical low-latency or high-throughput workloads
    - STI (HDD): Low cost HDD volume designed for frequently accessed, throughtput-intensive workloads
    - SCI (HDD): Lowest cost HDD volume designed for less frequently accessed workloads
- EBS volumes are characterized in Size | Throughput | IOPS
- When in douby always consult the AWS documentation


- GP2
    - Recommended for most workloads

- EBS Snapshots
    - Incremental - only backup changed blocks
    - EBS backups use IO and you shouldn'y run them while your application is handling a lot of traffic
    - Snapshots will be stored in S3 
    - Not necessary to detach volume to do snapshot, but recommended 
    - Max 100,000 snapshots
    - Can copy snapshots across AZ or Region
    - Can make image from Snapshot
    - EBS volumes restored by snapshots need to be pre-warmed


- EBS Migration
    - EBS volumnes are only locked to a specifc AZ
    - To migrate it to a different AZ
        - Snapshot the volume
        - Copy the volume to a different region
        - Create a volume from the snapshot in the AZ of your choice

- EBS Encryption
    - When you create an encrypted EBS volume, you get the following
        - Data at rest is encrypted inside the volume
        - All the data in flight moving between the instance and the volume is encrypted 
        - All snapshots are encrypted
        - All volumes created from the snapshot
    - Encryption and decryption are handled transparently
    - Encryption has a minimal impact on latency
    - EBS Encryption leverages keys from KMS
    - Copying an unencrypted snapshot allows encryption
    - Snapshots of encrypted volumes are encrypted


- EBS vs Instance Store
    - Some instance do not come with Root EBS volumes
    - Instead, they come with "Instance Store"
    - Instance store is physically attached to the machine (EBS is a network drive)
    - Pros
        - Better I/O performance
        - Good for buffer / cache / scratch data / temporary content
    - Cons
        - On stop or termination, the instance store is lost
        - You can't resize the instance store
        - Backup must be done 


- EBS RAID Options
    - EBS is already redundant storage 
    - But what if you want to increase IOPS to say 100 000 IOPS
    - You would mount volumes in parallel in RAID settings
    - RAID is possible as long as your OS supports it
    - Some RAID options are:
        - RAID 0
        - RAID 1
        - RAID 5 (not recommended)
        - RAID 6 (not recommended)
    
- RAID 0 (increase performance)
    - Combining 2 or more volumes and getting the total disk space and I/O
    - But one disk fails, all the data is failed
    - Use cases would be
        - An application that needs a lot of IOPS and doesn't need fault-tolerance
        - A database that has replication already built-in
    - Using this, we can have a very big disk with a lot of IOPS
    - e.g.
        - two 500 GiB EBS io | volumes with 4,000 provisioned IOPS each will create a
        - 1000 GiB RAID 0 array with an available bandwidth of 8,000 IOPS and 1,000 MB/s of throughput
    
- RAID 1 (increase fault tolerance)
    - Mirroring a volume to another
    - If one disk fails, our logical volume is still working
    - We have to send the data to two EBS volume at the same time
    - 

- EFS - Elastic File System
    - Managd NFS (network file system) that can be mounted on many EC2
    - EFS works with EC2 instances in multi-AZ
    - Highly available, scalable, expensive (3x gp2), pay per use
    - Use cases : content management, web serving, data sharing
    - Uses NFSv4.1 protocol
    - Uses security group to control access to EFS
    - Compatible with Linux based AMI (not Winrdows)
    - Performance mode:
        - General purpose 
        - Max I/O - used when thousands of EC2 are using the EFS
        - EFS file sync to sync from on-premise file system to EFS
        - Backup EFS-to-EFS (incremental - can choose frequency)
        - Encryption at rest using KMS

- EFS Hands on

- EBS & EFS for solutions architect 
    - EBS volumes can be attached to only one instance at a time
    - EBS volumes are locked at the AZ level
    - Migrating as EBS volume across AZ means first backing it up then recreating it in the other AZ
    - EBS backups use IO and you shouldn't run them while your application is handling a lot of traffic
    - Root EBS volumes of instances get terminated by default if the EC2 instance gets terminated

    - Disk IO is high => Increase EBS volume size
    - EFS mounting 100s of instances
    - EFS share website files
    - EBS gp2, optimize on cost
    - Custom AMI for faster deploy on ASG
    - EFS vs EBS vs Instance Store


- AWS RDS Overview
    - RDS Read Replicas for Read scalability
        - Up to 5 read replicas
        - within AZ, cross AZ or Cross Region
        - Replication is ASYNC, so reads are eventually consistent
        - Replicas can be promoted to their own DB
        - Applications must update the connection string to leverage read replicas
    - RDS Multi AZ
        - SYNC replication
        - One DNS name - automatic app failover to standby
        - Increase availability
        - Failover in case of loss of AZ, loss of network, instance or storage failure
        - No manual intervention in apps
        - Not used for scaling
    - RDS Backups
        - Backups are automatically enabled in RDS
        - Automated backups
            - Daily full snapshot of the database
            - Capture transaction logs in real time
            - ability to restore to any point in time
            - 7 days retention
        - DB snapshots
            - Manually triggered by the use
            - Retention of backup for as long as you want
    - RDS Encryption
        - Encryption at rest capability with AWS KMS - AES-256 
        - SSL certificates to encrypt data to RDS in flight
        - To enforce SSL
            - PostgreSQL : rds.force_sll=1 in the AWS RDS Console
            - MySQL : within the DB grant usage on *.* to 'mysqluser'@'%' Require ssl;
        - To connect using SSL
            - Provide the SSL Trust certificate
            - Provide SSL options when connecting to database
    - RDS Security
    - RDS vs Aurora

# AWS RDS Hands On

# RDS Security for SysOps
- Encryption at rest
    - Is done only when you first create the DB instance
    - or: unencrypted DB => Snapshot => copy snapshot as encrypted => create DB from snapshot
    - Your responsibility
        - Check the ports / IP / Security group inbound rules in DB's SG
        - in-database user creation and permissions
        - Creating a database with or without public access
        - Ensure parameter groups or DB is configured to only allow SSL connnections
    - AWS's
        - No SSH access
        - No manual DB patching
        - No manual Os patching
        - No way to audit the underlying instance

# RDS for Solutions Architects
- Read replicas are used for SELECT only kind of statements 
- Amazon RDS supports Transparent Data Encryption for DB encryption
    - Oracle or SQL server DB instance only
    - TDE can be used on top of KMS
- IAM authentication
    - works for MySQL, PostgreSQL
    - Lifespan of an authentication token is 15 minutes
    - Tokens are generated by AWS credentials
    - SSL must be used when connecting to the database
    - Easy to use EC2 instance Roles to connect to the RDS database

# Amazon Aurora

# Aurora Hands On

# Aurora for SA
- Can use IAM authentication for Aurora MySQL and Postgre
- Aurora Global databases span multiple regions and enable DR
    - One primary region
    - one DR region
    - the dr region can be used for lower latency reads
    - < 1 second replica lag on average
- If not using Global databases, you can create cross -region read replicas 
    - but the FAQ recommends you use Global Databases instead

# AWS ElasticCache Overview
- The same way RDS is to get managed Relational Databases
- ElastiCache is to ge managed Redis or Memcached
- Caches are in-memory databases with really high performance, low latency
- Helps reduce load off of databases for read intensive workloads
- Helps make your application stateless
- Write Scaling using sharding
- Read Scaling using Read Replicas
- Multi AZ with Failover Capability
- AWS takes care of OS maintenance / patching, optimizations, setup, configuration, monitoring, failure recovery and backups

- Usages
    - DB cache
    - User Session Store
        - User logs into any of the application
        - The application writes the session data into ElastiCache
        - The user hits another instance of our application
        - The instance retrieves the data and the user is already logged in

# ElastCache for SA
- Security
    - Redis support Redis AUTH (username / password)
    - SSL in-flight encrpytion must be enabled and used
    - Memcached support SASL authentication
    - None of the caches support IAM authentication
    - IAM policies on ElastiCache are only used for AWS api-level security
- Patterns for ElastiCache
    - Lazy Loading: All the read data is cached, data can become stale in cache
    - Write Through: Adds or update data in the cache when written to a DB(no stale data)
    - Session Store: store temporary session data in a cache (using TTL features)

- Route 53
    - Route 53 is a Managed DNS
    - DNS is a collection of rules and records which helps clients understand how to reach a server through URLs
    - In AWS, the most common records are:
        - A: URL to IPv4
        - AAAAL UR to IPv6
        - CNAME: URL to URL
        - Alias: URL to AWS resource

# DNS Records TTL


# CNAME vs Alias
- AWS Resources expose an AWS URL:
- CNAME
    - porints a URL to any other URL
    - ONLY FOR NON ROOT DOMAIN
- Alias
    - points a URL to an AWS Resource
    - Works for ROOT DOMAIN and NON ROOT DOMAIN
    - free of charge
    - health check

# Simple Routing Rolicy
- Maps a domain to one URL
- Use when you need to redirect to a single resource
- You can't attach health checks to simple routing policy

# Weighted Routing Policy
- Control the % of the reqeusts that go to specific endpoint
- Helpful to test 1% of traffic on new app version for example

# Latency Routing Policy
- Redirect to the server that has the least latency close to us
- Super helpful when latency of users is a priority
- Latency is evaluated in terms of user to designated AWS Region
- Germany may be directed to the US 

# Health Checks

# Failover
- 

# Geo Location Routing Policy
- Different From latency based
- this is routing based on use location
- here we specify : traffic from the UK should go to this specific IP
- Should create a "default" policy

# Multi Value Routing Policy
- Use when routing traffic to multiple resources
- Want to associate a Route 53 health checks with records
- Up to 8

# Route 53 as a Registrar
- A domain name registrar is an organization that manages the reservation of Internet domain names
- Famous names
    - GoDaddy
    - Google Domains


# Solution Architecture Discussion Overview
- cost, performance, reliability, security, operational excellence

# Beanstalk Overview
- ElasticBeanStalk is a developer centric view of deploying an application on AWS

# AWS S3 Encryption for Objects

# AWS CLI on EC2
- https://awspolicygen.s3.amazonaws.com/policygen.html

# AWS Policy Simulator

# AWS EC2 Instance Metadata
- It allows AWS EC2 instances to learn about themselves without using an IAM Role for that purpose
- The URL is http://169.254.169.254/latest/meta-data
- You can retrieve the IAM Role name from the metadata, but you CANNOT retrieve the IAM policy
- Metadata = info about the EC2 instance
- Userdata = launch script of the EC2 instance

# S3 MFA-DELETE

# S3 ACcess Logs
- 

# S3 Cross Region Replication
- 


# S3 presigned URLs
- 

# AWS CloudFront 
- Content Delivery Network
- Improves read performance, content is cached at the edge
- 136 point of Presence globally
- Popular with S3 but works with EC2, load balancing
- Can help protect against network attacks
- Can provide SSL encryption (HTTPS) at the edge using ACM
- CloudFront can use SSL encryption (HTTPS) to talk to your applications
- Support RTMP Protocol (videos / media)

# AWS CloudFront Hands On
- We'll create an S3 bucket
- We'll create on CloudFront distribution
- We'll create an Origin Access Identity
- We'll limit the S3 bucket to be accessed only using this identity


# CloudFront Signed URK / Signed Cookie
- Say you wanted to distribute paid shared content to premium users over the world, the conent lives in S3
- If S3 can only be accessed through CloudFront, we cannot used self-signed S3 URLs
- We can use CloudFront Signed URL. We attach a policy with
    - Includes URL expiration
    - Includes IP ranges to access the data from
    - Trusted signers
- CloudFront signed URL can only be created using the AWS SDK, so you have to code an application to verify users and generate these URLS
- How long should the URL be valid for?
    - Shared content: make it short

- CloudFront vs Cross REgion Replication
    - CloudFront
        - Global Edge network
        - files are cached for a TTL
        - Great for static content that must be available everywhere
    - S3 Cross Region Replication
        - Must be setup for each region you want replication to happen
        - Files are updated in near real-time
        - Read only
        - Great for dynamic content that needs to be available at low-latency in few regions

- CloudFront Geo Restriction
    - You can restrict who can access your distribution
    - Whitelist: allow your users to access your content only if they're in one of the coutries on a list of approved countries
    - Balcklist: Prevent your users from ccessing your content if they're in oneof the coutries on

# S3 Storage Tiers
- Amazon S3 Standard - General purpose
- IA (infrequent access)
- One Zone-IA
- Reduced Redundancy Storage
- Intelligent Tiering
- Glacier

- General Purpose
    - High durability
    - RRS
        - 99.99% Durability
        - 99.99% Availability
    - IA
        - Suitable for data that is less frequently accessed, but requires rapid access when needed
        - High Durability of objects across multiple AZs
        - 99.99% Availability
        - Low cost compared to Amazon S3 Standard
        - Sustain 2 concurrent facility failures
    - S3 One Zone IA
        - Same as IA but data is stored in a single AZ
        - High durability of objects in a single AZ; data lost when AZ is destroyed
        - 99.95 % Availability
        - Low cost compared to IA (by 20%)
    - S3 Intelligent Tiering
        - Same low latency and high throughput performance of S3 Standard
        - Automatically moves objects between two access tiers based on chaging access patterns
    - Glaciers
        - Archiving or backups

# S3 Lifecycle Rules
- 

# Snowball
- Physical data transport solution that helps moving TBs or PBs of data in or out of AWS
- Alternative to moving data over the network 
- Secure, tamper resistant, uses KMS 256 bit encryption
- Tracking using SNS and text message. E-int shipping label

1. Request snowball devices from the AWS console for delivery
2. Install the snowball client on your servers
3. Connect the snowball to your servers and copy files using the client
4. Ship back the device when you're done
5. Data will be loaded into an S3 bucket
6. Snowball is completely wiped
7. Tracking is done using SNS, text messages and the AWS console

- Snowball Edge
- AWS Snowmobile

# Storage Gateway for S3
- Hybrid Cloud for Storage
    - AWS is pushing for hybrid cloud
        - part of your infrastructure is on the cloud
        - and on-premise
    - This can be due to
        - Long cloud migrations
        - Security requirements
        - Compliance requirements
        - IT strategy
    - S3 is a proprietary storage technology, so how do you expose the S3 data on-premise?
    - AWS Storage gateway

- AWS Storage Cloud Native Options
    - Block
        - EBS, Instance Store
    - File
        - Amazon EFS
    - Object
        - S3, Glacier
    
- AWS Storage Gateway
    - Bridge between on-premise data and cloud data in S3
    - Use cases: DR, backup & restore, tiered storage

- 3 types of Storage Gateway
    - File Gateway
    - Volume Gateway
    - Tape Gateway

- File Gateway
    - Configured S3 buckets are accessible using the NFS and SMB protocol
    - Supports S3 standard, S3 IA, S3 One Zone IA
    - Bucket access using IAM roles for each File Gateway
    - Most recently used data is cached in the file gateway
    - Can be mounted on many servers

- Volume Gateway
    - Block storage using iSCSI protocol backed by S3
    - Backed by EBS snapshots which can help restore on-premise volumes
    - cached volumes: low latency access to most recent data
    - Stored volumes: entire dataset is on premise, scheduled backups to S3

- Tape Gateway
    - Some companies have backup preocesses using physical tapes
    - With Tape Gateway, companies use the same processes but in the cloud
    - Virtual Tape Library backed by Amazon S3 and Glacier
    - Back up data using existing tape-based processes
    - Works with leading backup software vendors

# Athena
- Serverless service to perform analytics directly against S3 files
- Uses SQL language to query the files
- Has a JDBS / ODBS driver
- Charged per query and amount of data scanned
- Supports CSV, JSON, ORC, Avro and Parquet
- Use cases: Business intelligence, analytics, reporting, analyze & query VPC flow logs, ELB logs, CloudTrail trails, etc

# Section Intro : Messaging
- There are two patterns of application communication
    - Synchronous
    - Asynchronous

- Synchronous between applications can be problematic if there are sudden spikes of traffic
- In that case, it's better to decouple your applications,
    - using SQS: queue model
    - using SNS: pub/sub model
    - using Kinesis: real-time streaming model

# AWS SQS
- SQS - Standard Queue
    - Oldest offering
    - Fully managed
    - Scales from 1 message per second to 10,000s per second
    - Default retention of messages: 4 days, maximum of 14 days
    - No limit to how many messages can be in the queue
    - Low latency
    - Horizontal scaling in terms of number of consumers
    - Can have duplicate messages
    - Can have out of order messages
    - Limitation of 256KB per message sent
- SQS - delay queue
    - Delay a message (consumers don't see it immediately) up to 15 minutes
    - Default is 0 seconds
    - Can set a default at queue level
    - Can override the default using the Delay Seconds parameter

- SQS - Producing Messages
    - Define Body
    - Add message attributes
    - Provide Delay Delivery
    - Get back
        - Message identifier
        - MD5 hash of the body
    
- SQS - Consuming Messages
    - Poll SQS for messages (receive up to 10 messages at a time)
    - Process the message within the visibility timeout
    - Delete the message using the message ID & receipt handle

- SQS - visilbility timeout
    - When a consumer polls a message from a queue, the message is "invisible" to other consumers for a defined period... the visibility timeout
        - Set between 0 seconds and 12 hours
        - If too high (15 minutes) and consumer fails to process the message, you must wait a long time before processing the message again
        - If yoo low (30 seconds) and consumer needs time to process the message (2 minutes), another consumer will receive the message and the message will be processed more than once
    - ChangeMessageVisibility API to change the visibility while processing a message
    - DeleteMessage API to tell SQS the message was successfully processed

- SQS - Dead Letter Queue
    - If a consumer fails to process a message within the visibility timeout, the message goes back to the queue
    - We can set a threshold of how many times a message can go back to the queue - it's caleld a redrive policy
    - After the threshold is exceeded, the message goes into a dead letter queue (DLQ)
    - We have to create a DLQ first and then designate it dead letter queue
    - Make sure to process the messages in the DLQ before they expire

- AWS SQS - Long Polling
    - When a consumer requests message from the queue, it can optionally "wait" for messages to arrive if there are none in the queue
    - This is called long polling
    - LongPolling decreases the number of API calls made to SQS while increasing the efficiency and latency of your application
    - The wait time can be between 1 sec to 20 sec
    - Long Polling is preferable to Short Polling
    - Long polling can be enabled at the queue level or at the API using WaitTimeSeconds

# AWS FIFO Queue
- Newer offering 
- Name of the queue must end in .fifo
- Lower throughput (up to 3,000 per second with batching 300/s without)
- messages are processed in order by the consuemr
- Messages are sent exactly once
- No per message delay
- Ability to do content based de-duplication
- 5-minute interval de-duplication using "Duplication ID"
- Message Groups:
    - Possibility to group messages for FIFO ordering using "Message GroupID"
    - Only one worker can be assigned per message group so the messages are processed in order
    - Message group is just an extra tag on the message 


# AWS SNS
- What if you want to send one message to many receivers?
    - Direct integration
    - Pub / Sub
- The "event producer" only sends message to one SNS topic
- As many "event receiver" as we want to listen to the SNS topic notifications
- Each subscriber to the topic will get all the messages
- Up to 10,000,000 subscriptions per topic
- 100,000 topics limit 
- Subscribers can be:
    - SQS
    - HTTP / HTTPS
    - Lambda
    - Emails
    - SMS messages
    - Mobile Notifications

- SNS intergrates with a lot of Amazon Products
    - Some services can send data directly to SNS for notifications
    - CloudWatch
    - Auto Scaling Groups Notifications
    - Amazon S3
    - CloudFormation

- AWS SNS - How to publish
    - Topic publish (withi your AWS server - using the SDK)
        - Create a topic
        - Create a subscription
        - Publish to the topic
    - Direct publish (for mobile apps SDK)
        - Create a platform application
        - Create a platform endpoint
        - Publish to the platform endpoint
        - Works with Google GCM, Apple APNS, Amazon ADM

- SNS + SQS: Fan Out
    - Push onve in SNS, receive in many SQS
    - Fully decoupled
    - No data loss
    - Ability to add receivers of data later
    - SQS allows for deplated processing
    - SQS allows for retries of work
    - May have many workers on one queue and one worker on the other queue

# AWS SNS hands on

# AWS Kinesis Overview
- Managed alternative to Apache Kafka
- Great for application logs, metrics, IoT, clickstreams
- Great for "real time" big data
- Great for streaming Processing frameworks (Spark, NiFi)
- Data is automatically replicated to 3AZ

- Kinesis Streams: low latency streaming ingest at scale
- Kinesis analytics: perform real-time analytics on streams using SQL
- Kinesis Firehose: load streams into S3, Redshift, ElasticSearch

- Kinesis Streams Overview
    - Streams are divided in ordered Shards / Partitions
    - Data retention is 1 day by default, can go up to 7 days
    - Ability to reprocess / replay data 
    - Multiple applications can consume the same stream
    - Real-time processig with scale of throughput
    - Once data is inserted in Kinesis, it can't be deleted

- Kinesis Streams Shards
    - One stream is made of many different shards
    - 1 MB/s or 1000 messaes/s at write per shard
    - 2 MB/s at read Per shard
    - billing is per shard provisioned, can have as many shards as you want
    - Batching available or per message calls
    - the number of shards can evelve over time (rechard / merge)
    - Records are ordered per shard

- AWS Kinesis API - put records
    - PutRecord API + Partition key that gets hashed
    - The same key goes to the same partition ( helps with ordering for a specific key)
    - Messages sent get a "sequence number"
    - Choose a partition key that is highly distributed (helps prevent "hot partition")
        - user_id if many users
        - Not country_id if 90% of the users are in one country
    - Use Batching with PutRecord to reduce costs and increase throughput
    - ProvisionedThroughputExceeded if we go over the limits
    - Can use CLI, AWS SDK, or producer libraries from various frameworks

- AWS Kinesis API - Exceptions
    - ProvisionedThroughputExceeded Exceptions
        - Happends when sending more data
        - Make sure you don't have a hot shard
    - Solution
        - Retries with backoff
        - Increase shards
        - Ensure your partition key is a good one
    
- AWS Kinesis API - Consumeres
    - Can use a normal consumer
    - Can use Kinesis Client Library
        - KCL uses DynamoDB to checkpoint offsets
        - KCL uses DynamoDB to track other workers and share the work amongst shards

# Kinesis Security
- Control Access / Authorization using IAM policies
- Encryption in flight using HTTPS endpoints
- Encryption at rest using KMS
- Possibility to encrypt / decryption data client side
- VPC Endpoints available for Kinesis to access within VPC

- AWS Kinesis Data Anlytics
    - Perform realtime analytics on Kinesis Streams using SQL
    - Kinesis Data Analytics
        - Auto scaling
        - Manged : no server  to provision 
        - Continuous: real time
    - pay for actual consumption rate
    - Can create streams out of the real time queries

- AWS Kinesis Firehose
    - Fully managed service
    - Near Real time
    - Load data into redshift / amanzon s3 / elesticsearch / splunk
    - Automatic scaling
    - Support many data format
    - Pay for the amount of data going through firehose

- SQS vs SNS vs Kinesis
    - SQS
        - Consumer pull data
        - Data is deleted after being consumed
        - can have as many workers as we want
        - No need to provision throughput
        - no ordering guarantee
        - individual message delay capability
    - SNS
        - Push data to many subsribers
        - Up to 10,000,000 subsribers
        - Data is not persisted
        - Pub/Sub
        - Up to 100,000 topics
        - No need to provision throughput
        - Integrates with SQS for fan-out architecture pattern
    - Kinesis
        - Consumers pull data
        - As many consumers as we want
        - Possibility to replay data
        - Meant for real-time big data analytics and ETL
        - Ordering at the shard level
        - Data expires after x days
        - Must provision throughput

# Amazon MQ
- SQS, SNS are "cloud native" services, and they're using proprietary protocols from AWS
- Traditional applications running from on-premise may use open protocols such as MQTT, AMQP, STOMP, Openwire, WSS
- When migrating to the cloud, instread of re-engineering the application to use SQS and SNS, we can use Amazon MQ
- Amazon MQ = managed Apache ActiveMQ

- Amazon MQ doesn't "scale" as much as SQS / SNS
- Amazon MQ runs on a dedicated machine, can run in HA with failover
- Amazon MQ has both queue feature (~SQS) and topic features
